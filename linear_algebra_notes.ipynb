{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM3Qwt8e4uO7vtBm3zibHX5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Whenning42/linear-algebra-notes/blob/main/linear_algebra_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "ikJixZXrbIsh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# William’s Linear Algebra Notes\n",
        "\n",
        "$\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}$\n",
        "\n",
        "## Latex Background\n",
        "\n",
        "An latex command is given between two `$` symbols. For example `$m \\times n$` renders as $m \\times n$.\n",
        "\n",
        "Here's a table of some common notation:\n",
        "\n",
        "| Name  | Rendered | Latex |\n",
        "|-------|--------------|------------|\n",
        "| Reals | $\\mathbb{R}$ | \\mathbb{R} |\n",
        "| Complex | $\\mathbb{C}$ | \\mathbb{C} |\n",
        "| Exponentiation | $a^b$, $a^{b+c}$ | a^b, a^{b+c}, note to put more than a yletter in the exponent, we use curly brackets. |\n",
        "| Element of | $a \\in S$ | a \\in S |\n",
        "| Transpose | $A^\\top$ | A^\\top |\n",
        "| Dot Product | $x \\cdot y$ | x \\cdot y |\n",
        "| Inner Product | $\\langle x, y \\rangle$ | \\langle x, y \\rangle |\n",
        "| Vector Norm | $\\norm{x} $ | \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} then \\norm{x} |\n",
        "| Orthogonal | $x \\perp y$ | x \\perp y |"
      ],
      "metadata": {
        "id": "1dEkCUUkXx8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrices\n",
        "\n",
        "A matrix is an $m \\times n$ rectangle consisting of $m$ rows and $n$ columns of scalar values from some field $\\mathbb{F}$ (commonly $\\mathbb{R}$ or $\\mathbb{C}$). We deonte the space of $m \\times n$ matrices as $\\mathbb{F}^{m \\times n}$.\n",
        "\n",
        "An example $2 \\times 3$ matrix $A$ over the reals is:\n",
        "$A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$\n",
        "\n",
        "### Latex\n",
        "\n",
        "We can write a bracketed matrix like this in latex:\n",
        "\n",
        "```\n",
        "A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n",
        "```"
      ],
      "metadata": {
        "id": "vTlqGWEKgk0M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b0I7d0NsXwX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c52f855-17a5-4e8c-cde8-0ef6b3a5692b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# Note: All libraries use row major notation.\n",
        "# Sets a to:\n",
        "# [[1, 2, 3]\n",
        "#  [4, 5, 6]]\n",
        "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(a)\n",
        "\n",
        "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(a)\n",
        "\n",
        "a = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(a)\n",
        "\n",
        "# TODO: Add some simple matrix constructors like eye or diag."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block Matrices\n",
        "\n",
        "A block matrix is a matrix that concatenates a rectangular grid of submatrices. Block matrices are denoted as:\n",
        "\n",
        "$M = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}$\n",
        "\n",
        "With concatenated matrices needing to match the size of their neighbors along their adjacent sides."
      ],
      "metadata": {
        "id": "ioW-9mRGiRLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy\n",
        "a = np.array([[2]])\n",
        "b = np.array([[4, 6]])\n",
        "c = np.array([[8]])\n",
        "d = np.array([[10, 12]])\n",
        "M = np.block([[a, b], [c, d]])\n",
        "print(M)\n",
        "\n",
        "# Torch\n",
        "#\n",
        "# Torch doesn't have a block matrix constructor.\n",
        "# We can however use `torch.cat().\n",
        "a = torch.tensor(a)\n",
        "b = torch.tensor(b)\n",
        "c = torch.tensor(c)\n",
        "d = torch.tensor(d)\n",
        "t = torch.cat([a, b], dim=1)\n",
        "b = torch.cat([c, d], dim=1)\n",
        "M = torch.cat([t, b], dim = 0)\n",
        "print(M)\n",
        "\n",
        "# Jax\n",
        "a = np.array([[2]])\n",
        "b = np.array([[4, 6]])\n",
        "c = np.array([[8]])\n",
        "d = np.array([[10, 12]])\n",
        "M = jnp.block([[a, b], [c, d]])\n",
        "print(M)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhVe1caWiqZx",
        "outputId": "9957de73-e346-49e6-b32a-a2baf8308899"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  4  6]\n",
            " [ 8 10 12]]\n",
            "tensor([[ 2,  4,  6],\n",
            "        [ 8, 10, 12]])\n",
            "[[ 2  4  6]\n",
            " [ 8 10 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Transpose\n",
        "\n",
        "The transpose of a matrix $A$ is denoted as $A^\\top$ and is the result of flipping that matrix across its diagonal.\n",
        "\n",
        "Formally $A^\\top$ is defined such that $A^\\top_{j,i} = A_{i,j} \\ \\forall i, j$.\n",
        "\n",
        "### Latex\n",
        "\n",
        "We use `^\\top` to indicate the transpose, so for example `A^\\top` gives us $A^\\top$."
      ],
      "metadata": {
        "id": "LWSDAG7_kQ31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A transpose can be applied in both the Numpy and Torch APIs either\n",
        "# by reordering axes or swapping two axes. We show both options for\n",
        "# both APIs. Do note the APIs use different names for these operations.\n",
        "\n",
        "# Numpy\n",
        "# `np.transpose` reorders axes.\n",
        "# `np.swapaxes()` swaps two axes.\n",
        "a = np.array([[1, 2]])\n",
        "print(np.transpose(a, axes=(1, 0)))\n",
        "print(np.swapaxes(a, 0, 1))\n",
        "\n",
        "\n",
        "# Torch\n",
        "# `torch.permute` reorders axes.\n",
        "# `torch.swapaxes()` swaps two axes.\n",
        "a = torch.Tensor([[1,  2]])\n",
        "print(torch.permute(a, (1, 0)))\n",
        "print(torch.transpose(a, 0, 1))\n",
        "\n",
        "# Jax\n",
        "a = jnp.array([[1, 2]])\n",
        "print(jnp.transpose(a, (1, 0)))\n",
        "print(jnp.swapaxes(a, 0, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb1_bSz7mx8G",
        "outputId": "4c84b587-bc9e-487f-c531-badc5ebe9826"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1]\n",
            " [2]]\n",
            "[[1]\n",
            " [2]]\n",
            "tensor([[1.],\n",
            "        [2.]])\n",
            "tensor([[1.],\n",
            "        [2.]])\n",
            "[[1]\n",
            " [2]]\n",
            "[[1]\n",
            " [2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Multiplication\n",
        "\n",
        "The product matrices $A$ and $B$ of size $m \\times n$ and $n \\times k$ respectively is denoted $AB$ and it's a matrix of size $m \\times k$ where:\n",
        "\n",
        "$AB_{ij} = A_i \\cdot B_{:,j}$"
      ],
      "metadata": {
        "id": "3K9jMPTV2jjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: Both numpy and torch use the `*` operator on matrices to be an\n",
        "# elementwise product, not a matrix multiply.\n",
        "a = np.array([[1, 5], [2, 6]])\n",
        "b = np.array([[1, 0], [0, 2]])\n",
        "print(\"A\\n\", a)\n",
        "print(\"B\\n\", b)\n",
        "print(\"AB\")\n",
        "\n",
        "# Numpy\n",
        "# A matrix multiply can be performed with `np.matmul()`, the `@` operator, or an\n",
        "# einsum.\n",
        "print(a @ b)\n",
        "print(np.matmul(a, b))\n",
        "print(np.einsum('ij,jk->ik', a, b))\n",
        "\n",
        "# Torch\n",
        "a = torch.tensor(a)\n",
        "b = torch.tensor(b)\n",
        "print(a @ b)\n",
        "print(torch.matmul(a, b))\n",
        "print(torch.einsum('ij,jk->ik', a, b))\n",
        "\n",
        "# Do note that Numpy's einsum is much slower that matmul since it's a python\n",
        "# implementation whereas Torch and Jax's einsum are close in speed to\n",
        "# their matmuls.\n",
        "a = np.ones((500, 500))\n",
        "b = np.ones((500, 500))\n",
        "print(\"Numpy matmul vs einsum:\")\n",
        "%timeit np.matmul(a, b)\n",
        "%timeit np.einsum('ij,jk->ik', a, b)\n",
        "a = torch.tensor(a)\n",
        "b = torch.tensor(b)\n",
        "print(\"Torch matmul vs einsum:\")\n",
        "%timeit torch.matmul(a, b)\n",
        "%timeit torch.einsum('ij,jk->ik', a, b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaEcO2GB5nwc",
        "outputId": "14ee7ca2-de2f-4dc3-a98a-47c04145631f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            " [[1 5]\n",
            " [2 6]]\n",
            "B\n",
            " [[1 0]\n",
            " [0 2]]\n",
            "AB\n",
            "[[ 1 10]\n",
            " [ 2 12]]\n",
            "[[ 1 10]\n",
            " [ 2 12]]\n",
            "[[ 1 10]\n",
            " [ 2 12]]\n",
            "tensor([[ 1, 10],\n",
            "        [ 2, 12]])\n",
            "tensor([[ 1, 10],\n",
            "        [ 2, 12]])\n",
            "tensor([[ 1, 10],\n",
            "        [ 2, 12]])\n",
            "Numpy matmul vs einsum:\n",
            "30.4 ms ± 7.42 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "101 ms ± 27.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "Torch matmul vs einsum:\n",
            "18.3 ms ± 6.41 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "11.6 ms ± 1.11 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Angle Between Vectors\n",
        "\n",
        "The angle between two vectors $x$ and $y$ is given by the equation\n",
        "\n",
        "$\\cos(\\theta) = \\frac{\\langle x, y \\rangle}{\\norm{x}\\norm{y}}$\n",
        "\n",
        "and so the angle can be found with this formula\n",
        "\n",
        "$\\theta = \\cos^{-1}\\left(\\frac{\\langle x, y \\rangle}{\\norm{x}\\norm{y}}\\right)$\n",
        "\n",
        "Note: When implementing this equation, two edge cases need to be handled:\n",
        "1. If $\\norm{x} = 0$ or $\\norm{y} = 0$, then the angle is undefined. In this case returning a ValueError is most appropriate.\n",
        "2. If due to rounding errors, $\\frac{\\langle x, y \\rangle}{\\norm{x}\\norm{y}}$ is $\\lt -1$ or is $\\gt 1$, than $\\cos^{-1}$ will fail. In this case, clipping the fraction to between $-1$ and $1$ is an easy fix.\n",
        "\n",
        "\n",
        "#### Orthogonal Vectors\n",
        "\n",
        "Orthogonal vectors are vectors $x$ and $y$ such that $\\langle x, y \\rangle = 0$. We denote two vectors being orthogonal as $x \\perp y$ or in Latex, `x \\perp y`.\n",
        "\n",
        "Due to rounding errors, the inner product often won't be exactly $0$ and in which case we can consider two vectors orthogonal numerically if their normed inner product is less than some $\\epsilon$.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ua9_Mjh5S8k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The implementation of vector angle (at least in the non-vectorized case)\n",
        "# is the same for numpy, torch, and jax.\n",
        "\n",
        "# Numpy\n",
        "a = np.array([2, 0, 2])\n",
        "b = np.array([2, 0, -2])\n",
        "\n",
        "# TODO: How do you do batched bounds checking?\n",
        "def np_vec_angle(a, b, eps=1e-10):\n",
        "  norm_a = np.linalg.norm(a)\n",
        "  norm_b = np.linalg.norm(b)\n",
        "  if norm_a < eps or norm_b < eps:\n",
        "    raise ValueError(\"np_vec_angle doesn't support sub-epsilon length vectors\")\n",
        "\n",
        "  normed_dot = np.dot(a, b) / (norm_a * norm_b)\n",
        "  return np.arccos(np.clip(normed_dot, -1, 1))\n",
        "print(np_vec_angle(a, b))\n",
        "\n",
        "# Torch\n",
        "def torch_vec_angle(a, b, eps=1e-10):\n",
        "  # `torch.norm()` requires a float typed tensor. This function accepts any\n",
        "  # data type that's convertable to floats and does the conversion itself.\n",
        "  a = a.float()\n",
        "  b = b.float()\n",
        "  norm_a = torch.norm(a)\n",
        "  norm_b = torch.norm(b)\n",
        "  if norm_a < eps or norm_b < eps:\n",
        "    raise ValueError(\"torch_vec_angle doesn't support sub-epsilon length vectors\")\n",
        "\n",
        "  normed_dot = torch.dot(a, b) / (norm_a * norm_b)\n",
        "  return torch.acos(torch.clip(normed_dot, -1, 1))\n",
        "a = torch.tensor(a)\n",
        "b = torch.tensor(b)\n",
        "print(torch_vec_angle(a, b))\n",
        "\n",
        "# Jax\n",
        "a = jnp.array(a)\n",
        "b = jnp.array(b)\n",
        "def jnp_vec_angle(a, b, eps=1e-10):\n",
        "  norm_a = jnp.linalg.norm(a)\n",
        "  norm_b = jnp.linalg.norm(b)\n",
        "  if norm_a < eps or norm_b < eps:\n",
        "    raise ValueError(\"jnp_vec_angle doesn't support sub-epsilon length vectors\")\n",
        "\n",
        "  normed_dot = jnp.dot(a, b) / (norm_a * norm_b)\n",
        "  return jnp.arccos(np.clip(normed_dot, -1, 1))\n",
        "print(jnp_vec_angle(a, b))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CilpcV3EaAFJ",
        "outputId": "5c611c52-9a78-4644-cb77-750e96ccf41a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5707963267948966\n",
            "tensor(1.5708)\n",
            "1.5707964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Orthogonal Matrices\n",
        "\n",
        "A square matrix $Q \\in \\mathbb{R}^{n \\times n}$ is orthogonal or orthonormal iff $Q^\\top Q = QQ^\\top = I$.\n",
        "\n",
        "Due to rounding errors this equality will not generally exactly hold. Instead, numerically we can consider a matrix $Q$ orthogonal if $QQ^\\top - I \\approx 0$.\n",
        "\n",
        "The rows and columns of an orthogonal matrix $Q$ for an orthonormal basis of $\\mathbb{R}^n$. This follows directly the definition of orthogonal matrices and the matrix product.\n",
        "\n",
        "Specifically: $Q^\\top Q = I \\implies \\langle Q_i, Q_j \\rangle = \\langle Q_{:,i}, Q_{:, j}\\rangle = \\begin{cases} 1 & i = j \\\\ 0 & i \\neq j \\end{cases}$ \\\\\n",
        "Therefore, the rows and columns of $Q$ are an orthonormal basis of $\\mathbb{R}^n$."
      ],
      "metadata": {
        "id": "dV-W1wbIfDWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an orthogonal matrix\n",
        "# Verify QQ^T ~= I\n",
        "# Perturb Q, verify QQ^T !~= I\n",
        "\n",
        "# We'll use the QR factorization of an example matrix to construct an\n",
        "# orthogonal matrix Q.\n",
        "\n",
        "# Numpy\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "Q, R = np.linalg.qr(A)\n",
        "Q_T = np.swapaxes(Q, 0, 1)\n",
        "Qp = Q + np.array([[.05, 0], [0, 0]])\n",
        "Qp_T = np.swapaxes(Qp, 0, 1)\n",
        "\n",
        "print(\"Q is orthogonal:\", np.allclose(Q @ Q_T, np.eye(2)))\n",
        "print(\"Q's max delta from I:\\n\", Q @ Q_T - np.eye(2))\n",
        "print(\"Qp is orthogonal:\", np.allclose(Qp @ Qp_T, np.eye(2)))\n",
        "print(\"Qp's max delta from I:\\n\", Qp @ Qp_T - np.eye(2))\n",
        "print()\n",
        "\n",
        "# Torch\n",
        "Q = torch.tensor(Q).float()\n",
        "Qp = torch.tensor(Qp).float()\n",
        "Q_T = torch.transpose(Q, 0, 1)\n",
        "Qp_T = torch.transpose(Qp, 0, 1)\n",
        "print(\"Q is orthogonal:\", torch.allclose(Q @ Q_T, torch.eye(2)))\n",
        "print(\"Q's max delta from I:\\n\", Q @ Q_T - torch.eye(2))\n",
        "print(\"Qp is orthogonal:\", torch.allclose(Qp @ Qp_T, torch.eye(2)))\n",
        "print(\"Qp's max delta from I:\\n\", Qp @ Qp_T - torch.eye(2))\n",
        "print()\n",
        "\n",
        "\n",
        "# Jax\n",
        "A = jnp.array([[1, 2], [3, 4]])\n",
        "Q, R = jnp.linalg.qr(A)\n",
        "Q_T = jnp.swapaxes(Q, 0, 1)\n",
        "Qp = Q + jnp.array([[.05, 0], [0, 0]])\n",
        "Qp_T = jnp.swapaxes(Qp, 0, 1)\n",
        "print(\"Q is orthogonal:\", jnp.allclose(Q @ Q_T, jnp.eye(2), atol=1e-6))\n",
        "print(\"Q's max delta from I:\\n\", Q @ Q_T - jnp.eye(2))\n",
        "print(\"Qp is orthogonal:\", jnp.allclose(Qp @ Qp_T, jnp.eye(2), atol=1e-6))\n",
        "print(\"Qp's max delta from I:\\n\", Qp @ Qp_T - jnp.eye(2))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dmRXsHkfCzS",
        "outputId": "07833951-0aa6-4178-c246-51c56b871dd8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q is orthogonal: True\n",
            "Q's max delta from I:\n",
            " [[ 0.0000000e+00 -5.8339229e-18]\n",
            " [-5.8339229e-18  0.0000000e+00]]\n",
            "Qp is orthogonal: False\n",
            "Qp's max delta from I:\n",
            " [[-0.02912278 -0.04743416]\n",
            " [-0.04743416  0.        ]]\n",
            "\n",
            "Q is orthogonal: True\n",
            "Q's max delta from I:\n",
            " tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "Qp is orthogonal: False\n",
            "Qp's max delta from I:\n",
            " tensor([[-0.0291, -0.0474],\n",
            "        [-0.0474,  0.0000]])\n",
            "\n",
            "Q is orthogonal: True\n",
            "Q's max delta from I:\n",
            " [[1.1920929e-07 6.4604976e-08]\n",
            " [6.4604976e-08 0.0000000e+00]]\n",
            "Qp is orthogonal: False\n",
            "Qp's max delta from I:\n",
            " [[-0.02912271 -0.04743412]\n",
            " [-0.04743412  0.        ]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "- add batching to our examples?\n",
        "  - I'm not sure which of the functions in this notebook would might be non-obvious in terms of how they generalize to batched matrices.\n",
        "- vec op\n",
        "- invertability (above angle between vectors)\n",
        "- determinant\n",
        "- eigenvalues\n",
        "- eigenvectors\n",
        "- trace\n",
        "- chapter 2 exercises?"
      ],
      "metadata": {
        "id": "VCfxwaTv7rO1"
      }
    }
  ]
}